---
title: "fMRI protocol optimisation for studying the basal ganglia and cortex during performance on a visual cueing task"
author: "Kelly G. Garner, Christopher R. Nolan, Markus Barth, Sakia Bollmann, Ole Jensen and Marta I. Garrido"
date: '`r format(Sys.time())`'
output:
#   bookdown::pdf_document2:
#     includes:
#       before_body: ../template/doc_prefix.tex 
#       in_header: ../template/preamble.tex
#     keep_tex: yes
#     latex_engine: xelatex # may need to change this
#     number_sections: no
#     toc: no
#   bookdown::html_document2:
#     number_sections: no
#     theme: readable
#     toc: yes
#   bookdown::tufte_html2:
#     number_sections: no
#     toc: yes
#   bookdown::word_document2: null
# fontsize: 12pt
# linestretch: 1.5
# link-citations: yes
# csl: https://raw.githubusercontent.com/citation-style-language/styles/master/chicago-annotated-bibliography.csl
# bibliography: ../template/library.bib
# always_allow_html: yes
# links-as-notes: true
---

```{r knitr_options, echo=FALSE}
library(knitr)
#rstudio will set the folder where .Rmd file seats as work directory
#set it back to the folder where .Rproj seats
opts_knit$set(root.dir = normalizePath("../"))
opts_chunk$set(fig.align = 'center', cache = FALSE, warning = FALSE,
  message = TRUE, echo = FALSE)
options(digits = 3, width = 88, knitr.graphics.auto_pdf = TRUE,
        knitr.kable.NA = '')
# download template files if not available
tpl_1 = 'https://raw.githubusercontent.com/daijiang/workflow_demo/master/template/preamble.tex'
tpl_2 = 'https://raw.githubusercontent.com/daijiang/workflow_demo/master/template/doc_prefix.tex'
# bib_1 = 'https://raw.githubusercontent.com/daijiang/workflow_demo/master/template/ref.bib'
# change directory accordingly
if(!file.exists(tpl_1f <- '../template/preamble.tex')) download.file(tpl_1, tpl_1f)
if(!file.exists(tpl_2f <- '../template/doc_prefix.tex')) download.file(tpl_2, tpl_2f)
if(knitr::is_latex_output() | knitr::is_html_output()){
  library(kableExtra)
} else {
  options(kableExtra.auto_format = FALSE) # for docx
}
```

```{r, libraries, echo=FALSE, message=F, warning=F}
#library(rmarkdown)    # You need this library to run this template.
#library(epuRate) 
library(magrittr)
library(tidyverse)
library(cowplot)
library(readr)
library(wesanderson)
#library(rjson)
library(RJSONIO)
library(cowplot)
library(rlang)
library(kableExtra)
library(scales)
#library(lme4) # for mixed effects modelling
source("../R/R_rainclouds.R") # for the raincloud plot
#source("R/R_rainclouds.R") 
```



**Running headline**: Environment and species richness

**Abstract**: Your awesome abstract here.

\clearpage

# Introduction

```{r aims, fig.align='center', out.width="600pix", fig.cap="Regions of interest. A) Basal Ganglia anterior view, B) Basal Ganglia posterior view, C) Cortical ROIs. CN = Caudate Nucleus, GPe = Globus Pallidus External, GPi = Globus Pallidus Internal, Put = Putamen, STN = Subthalamic Nucleus, VS = Ventral Striatum, FEF = Frontal Eye Fields, IPS = Intraparietal Sulcus, LOC = Lateral Occipital Complex"}
paradigm.fig.pth <- '../images/Fig_ROIs.png' 
#paradigm.fig <- readPNG(paradigm.fig.pth, native=TRUE, info=TRUE)
include_graphics(paradigm.fig.pth)

```

# Methods

<br>

## Participants

<br>

A total of 5 participants [1 female, 1 left-handed, mean age: 30.6 yrs, std: 7.7] took part in the experiment. Participants had normal or corrected-to-normal vision, and all reported no major neurological or psychiatric diagnoses, nor did they report any use of psychoactive medications. All participants received 20 AUD per hour for participation. Participants also earned cash rewards, contingent on the speed of correct responses in the presence of value cues (~15 AUD per session). The University of Queensland Human Research Ethics Committee approved the study as being within the guidelines of the National Statement on Ethical Conduct in Human Research and all participants gave informed, written consent.

<br>

## Experimental Protocols

<br>

Participants attended four experimental sessions; an initial behavioural session, where participants learned the the task procedures, and three sessions in the MRI scanner. Only the behavioural procedures performed in the scanner sessions are discussed here. However a full description of the initial behavioural session can be found online [here](). 

<br>

### Behavioural Paradigm

<br>

We used an adapted version of a spatial cueing task where probabilistic spatial and incentive value cues are presented in relation to an upcoming orientation judgement [@garnerIncentiveValueSpatial2021]. 

<br>

#### Apparatus

<br>

insert info about showee and matlab etc

<br>

#### Stimuli

<br>

A grey cross [RGB: 115, 115, 115] superimposed over a rotated white square [RGB: 191, 191, 191, 45$^\circ$] was presented in the centre of the screen. The grey cross subtended 0.8$^\circ$ visual angle, and the square (diamond) subtended 1.6$^\circ$. A leftward or rightward facing triangle [RGB: 90, 90, 90], superimposed on one half of the white diamond served as the informative spatial cue (p=.8), whereas simultaneous presentation of both triangles served as the uninformative spatial cue (p=.5). Note: the two triangles together comprised 80 % of the surface area of the underlying white square. Two coloured discs ($^\circ$ in diameter, matched for luminance; [gold RGB: 182, 133, 58, rose RGB: 230, 93, 85]) served as value cues. These were presented 5.5$^\circ$ from the centre along, and 2$^\circ$ below  the horizontal meridian. Further information on the value configurations are presented in the procedure section below. Targets were a gabor (3$^\circ$ in diameter, standard deviation of the Gaussian envelope, 0.3; spatial frequency, 2.5/ppd (pixels per degree)) oriented 45$^\circ$ clockwise or counter-clockwise, whereas distractors shared the same visual features except they were presented on one of the cardinal axes. The target and distractors were presented within the discs formed by the value cues. The distance of the target and distractor from the centre allowed the viewer to discriminate the gabors without making an eye-movement while remaining just within the locus of high-acuity vision (~6$^\circ$). The target and distractors were followed by mask stimuli, which consituted a patch of pixels randomly weighted between the background grey and white, weighted by the same Gaussian envelope as was used for the target and distractor. Feedback was presented in yellow when reward had been accrued [RGB: 255, 215, 0], and in grey [RGB: 90, 90, 90] for errors. Reward values were presented centrally in alphanumeric characters, and subtended $^\circ$. All stimuli were presented on a grey [RGB: 128, 128, 128] background.

<br>

#### Behavioural Procedure

<br>

As shown in Figure 2, each trial began with the presentation of the central fixation display (cross and diamond). The duration of this display varied across the protocols as we adjusted visual display onsets to coincide with the TR pulses sent from the MRI scanner ($a_t$; P1: 1100 ms, P2: 2020 ms, P3: 2840 ms). The duration of the remaining visual events was the same across all three protocols. Subsequently, the fixation cross was removed and the value cues onset. After 400 ms, the spatial cue was presented for 300 ms. The target and distractor were presented after a random interval between 0-100 ms after spatial cue offset (selected from a uniform distribution). The target and distractor were displayed for 67 ms, and were followed by the masks which were displayed for 67 ms. The orientation of the target (45$^\circ$ clockwise or counterclockwise) and the distractor (vertical or horizontal) was fully counterbalanced across target-distractor pairing, location, and cueing condition. The value cues and central diamond remained on display during the response interval [$b_t$; P1: 1826-1926 ms, P2: 2136-2146 ms, P3: 946 - 1046 ms]. Next, reward feedback was presented contingent on the participant's response. The duration of the feedback display was 1000 ms for both correct and incorrect responses. For correct responses, the maximum reward value available was weighted ($w$) by the proportion of the participant's response time (RT) of a given range (350 ($RT_{min}$) - 850 ($RT_{max}$) ms) - i.e. $w= \frac{RT_t - RT_{min}}{RT_{max} - RT_{min}}$. For the reward feedback display, the previous reward total was displayed on the left followed by a plus sign and the reward value attained on that trial to the right. Over the next second, the new reward value counted down as the previous reward value increased, over 50 ms intervals. In the case of an incorrect response, the previous points total was displayed in dark grey in the centre of the screen. 

Each value cue colour signalled the probability of attaining a high value reward, relative to attaining a low value reward, should the target appear at that location (see Figure 2, Panel B). For example, the gold colour could predict a high reward value with p = .8, and therefore signalled a higher expected reward value ($H$). In contrast, the rose colour would predict a high reward value with p = .2, and would therefore signal a lower expected reward value ($L$). Value cues were presented in 4 different possible combinations, which where fully counterbalanced over location (left vs right), and from now are coded in relation to the location where the upcoming target and distractor appeared; 1) both the target and distractor locations could contain the high expected reward value colour ($H_{tgt}/H_{dst}$), 2) both locations could contain the low expected reward value colour ($L_{tgt}/L_{dst}$), 3) the target location could contain the high expected reward value colour, whereas the distractor location contained the low expected value colour ($H_{tgt}/L_{dst}$), or 4) vice-versa ($L_{tgt}/H_{dst}$). Participants learned the colour-reward mappings in the previous behavioural session, colour-reward mappings were counterbalanced over participants. 

Directional spatial cues (see Figure 2, panel C) carried a p = ~.8 (.778) chance of validly signalling the upcoming target location. Bidirectional cues signaled that the target could appear in either location with p = .5. Across each functional run, there were 72 trials containing a directional cue (56 valid, 16 invalid), and 56 trials containing a bidirectional cue. Laterality of target presentation was fully counterbalanced across all cue conditions. Note that in the fMRI data we regress out variance linearly attributable to invalid trials, given their low number and higher susceptibility to variance differences to the other conditions, and perform contrasts to compare between the valid (p=.8) and bidirectional (p=.5) conditions.  

The reward available was matched between valid directional and bidirectional trials, so that the same number of likely (p = .8) and unlikely (p = .2) reward outcomes occurred in each spatial cueing condition. As there were not enough invalid trials to fully counterbalance whether each of the 4 reward configurations resulted in a likely (p = .8) or unlikely (p = .2) reward outcome, we counterbalanced over expected value _per se_. For example, over all invalid trials, the low expected value cue would result in the same proportion of likely and unlikely outcomes, however, this was independent to the exact value configuration that the cue appeared in (i.e. $L_{TGT}/L_{DST} | L_{TGT}/H_{DST}$).

Participants had received extensive instructions and training in the aforementioned task, the details of which can be found [here](). In addition to these instructions, participants were requested to remain as still as possible during the functional runs.

<br>

### Scanning Procedures

<br>

Each participant was scanned over three sessions, on a 7T MRI system (Siemens Healthineers, Germany) using a 32-channel phased array headcoil (Nova Medical, Wilmington, US). Within each session, participants completed 3 functional runs, 1 for each protocol (which are now referred to as P1, P2 & P3). The order of the protocols was counterbalanced across sessions. All sessions took place at the same time of day for each participant. Each session started with a short localizer scan, the anatomical scan, and then the 3 functional runs. The functional runs varied slightly in duration, owing to the differences in the inter-trial-interval, which was dependent on the TR of each protocol (P1 = 10:52 mins, P2 = 13:02, P3 = 12:44).

Participant #1 completed only the first two sessions, due to scheduling error. For participant #3, P3 was stopped early in the third scanning session, rendering it impossible to subsequently build the images. All remaining data was included in the analysis.


```{r paradigm, fig.align='center', out.width="600pix", fig.cap="Cueing paradigm. A) Task and trial sequence, B) value- and C) spatial-cue contingencies, D) Predicted behaviour (IE=inverse efficiency)"}
paradigm.fig.pth <- '../images/Fig_Paradigm.png' 
#paradigm.fig <- readPNG(paradigm.fig.pth, native=TRUE, info=TRUE)
include_graphics(paradigm.fig.pth)

```


<br>

#### MR Protocols

<br>

The details for each of the protocols are as follows; Protocol 1 (P1, multi-echo, multiband), TR = 700 ms, TE = 10 ms and 30.56 ms, 2 mm isotropic, GRAPPA 2 in phase encoding direction, excitation flip angle = 35$^\circ$, receiver bandwidth = 1930 Hz/pixel, partial Fourier 5/8, FOV = 192 mm x 192 mm x 96 mm, 48 slices, P2: (single-echo multiband), TR = 1510, TE = 19.4 ms, 1.5 mm isotropic, GRAPPA 3 in phase encoding direction, excitation flip angle 60$^\circ$, receiver bandwidth = 1116 Hz/pixel, partial Fourier 6/8, FOV = 192 mm x 192 mm x 122 mm, 81 slices, P3: (3D EPI) TR = 60 ms (Effective TR = 1920 ms), TE = 22 ms, 1.5 mm isotropic, GRAPPA 2 in phase encoding direction, excitation flip angle = 15$^\circ$, receiver bandwidth = 1116 Hz/pixel, partial Fourier 6/8, FOV - 192 mm x 192  mm x 120 mm, 80 slices. 

The MP2RAGE was acquired with the following parameters: TR = 4300 ms, TE = 3.38 ms, 0.8 mm isotropic, T1 1 = 840 ms, T1 2 = 2370 ms, flip angle 1 = 5$^\circ$, flip angle 2 = 6$^\circ$, FOV = 240 mm x 225 mm x 192 mm. 

```{r acquistable, echo=FALSE, message=F, warning=F}

# Show it:
TRs <- data.frame(TR=c(700, 1510, 1920), 
                  TE=c('10, 30.56','19.4','22'),
                  VoxSizeIso = c(2, 1.5, 1.5),
                  PhaseEncodingDirGRAPPA = c('2','3','2'),
                  MultiBandFactor=c('4','3','CAiPI shift 1'),
                  ExcitFlipAngle=c(35,60,15),
                  ReceiverBand_Hz_Px=c(1930,1116,1116),
                  partFourier=c('5/8','6/8','6/8'),
                  FOV=c('192 x 192 x 96','192 x 192 x 122','192 x 192 x 120'),
                  Nslices=c(48,81,80))
rownames(TRs) <- c("P1", "P2", "P3")
kable(t(TRs), caption="Table 1. Compared Protocols") 
#%>% kable_classic(full_width=F, html_font="Cambria")
```

<br>

### Preprocessing

<br>

All functional data were preprocessed using _fMRIprep_ (version 1.5.8; [estebanFMRIPrepRobustPreprocessing2019]), which is based on _Nipype 1.4.1 [@gorgolewskiNipypeFlexibleLightweight2011, @estebanNipyNipype2020]. 

**Anatomical data preprocessing** 
Each T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) with `N4BiasFieldCorrection` [@tustisonN4ITKImprovedN32010], distributed with ANTs 2.2.0 [@avantsSymmetricDiffeomorphicImage2008, RRID:SCR_004757]. The T1w-reference was then skull-stripped with a *Nipype* implementation of the `antsBrainExtraction.sh` workflow (from ANTs), using OASIS30ANTs as target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w using `fast` [FSL 5.0.9, RRID:SCR_002823, @zhangSegmentationBrainMR2001]. A T1w-reference map was computed after registration of 3 T1w images (after INU-correction) using `mri_robust_template` [FreeSurfer 6.0.1, @reuterHighlyAccurateInverse2010]. Brain surfaces were reconstructed using `recon-all` [FreeSurfer 6.0.1, RRID:SCR_001847, @daleCorticalSurfaceBasedAnalysis1999], and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical gray-matter of Mindboggle [RRID:SCR_002438, @kleinMindbogglingMorphometryHuman2017]. Volume-based spatial normalization to one standard space (MNI152NLin2009cAsym) was performed through nonlinear registration with `antsRegistration` (ANTs 2.2.0), using brain-extracted versions of both T1w reference and the T1w template. The following template was selected for spatial normalization: *ICBM 152 Nonlinear Asymmetrical template version 2009c* [@fonovUnbiasedNonlinearAverage2009, RRID:SCR_008796; TemplateFlow ID: MNI152NLin2009cAsym]. 

**Functional data processing**
For each BOLD run, the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of *fMRIPrep*. A deformation field to correct for susceptibility distortions was estimated based on *fMRIPrep*'s *fieldmap-less* approach. The deformation field is that resulting from co-registering the BOLD reference to the same-subject T1w-reference with its intensity inverted [@wangComparisonImageIntensity2017; @huntenburgLaminarPythonTools2017]. Registration is performed with `antsRegistration` (ANTs 2.2.0), and the process regularized by constraining deformation to be nonzero only along the phase-encoding direction, and modulated with an average fieldmap template [@treiberCharacterizationCorrectionGeometric2016]. Based on the estimated susceptibility distortion, a corrected EPI (echo-planar imaging) reference was calculated for a more accurate co-registration with the anatomical reference. The BOLD reference was then co-registered to the T1w reference using
`bbregister` (FreeSurfer) which implements boundary-based registration [@greveAccurateRobustBrain2009]. Co-registration was configured with six degrees of freedom. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using `mcflirt` [FSL 5.0.9, @jenkinsonImprovedOptimizationRobust2002]. BOLD runs were slice-time corrected using `3dTshift` from AFNI 20160207 [@coxSoftwareToolsAnalysis1997, RRID:SCR_005927]. The BOLD time-series, were resampled to surfaces on the following spaces: *fsaverage*, *fsnative*. The BOLD time-series (including slice-timing correction) were resampled onto their original, native space by applying a single, composite transform to correct for head-motion and susceptibility distortions. These resampled BOLD time-series will be referred to as *preprocessed BOLD in original space*, or just *preprocessed BOLD*. The BOLD time-series were resampled into standard space, generating a *preprocessed BOLD run in 'MNI152NLin2009cAsym' space*. First, a reference volume and its skull-stripped version were generated using a custom methodology of *fMRIPrep*. Several confounding time-series were calculated based on the *preprocessed BOLD*: framewise displacement (FD), DVARS and three region-wise global signals. FD and DVARS are calculated for each functional run, both using their implementations in *Nipype* [following the definitions by @power_fd_dvars]. The three global signals are extracted within the CSF, the WM, and the whole-brain masks.

<br>

### Single-subject analysis

<br> 

Processing performed subsequent to running *fMRIprep* was completed using a computing environment defined by [a singularity container for which the recipe is available online](https://github.com/kel-github/code-4-seq-comp-test-7T/blob/master/Singularity). All remaining processing was implemented using Nipype (version 1.6.0), ANTs 2.2.0, and SPM12-r7219, using [jupyter notebooks that are available online](https://github.com/kel-github/code-4-seq-comp-test-7T). 

First, the multi-echo images were combined within each session, using a weighted summation as defined in Puckett et al, [-@Puckett2018-wy]. Specifically, weights were calculated per voxel, $w_n$, using the following:

<br>

$w_n = \frac{AVG_n \cdot TE_n}{\sum AVG_n \cdot TE_n}$

<br>

where $AVG$ is the temporal average of the time-series. This weighting scheme carries the advantage that the weights can be directly estimated from the data, and no additional calibration scans or model assumptions are required. Moreover, this method has been shown to work as equally well as other combination schemes [@Kettinger2016-of]. **The weighted summation was implemented using the functionality of `fslmaths` [FSL 5.0.10, @Jenkinson2012-nl] CHNAGE THIS**. Next, a highpass filter was applied to all the preprocessed BOLD (cut-off 1/128 Hz) to remove slow drifts. As we sought to examine signal quality in the _a priori_ defined regions of interest, no spatial smoothing was applied to the data. For each participant and protocol, a first-level GLM was fit using the canonical double-gamma haemodynamic response function (HRF) [@Friston1994-kv, @Friston1998-vq, @Friston1994-yr, @Glover1999-aw] with time and dispersion derivatives. The design matrix contained regressors for the onset of each configuration of value cues, spatial cue laterality (left vs right) and probability (left p =.5, & p = .8), target location (left vs right), reward received (high vs low by its probability, p = .2 & p = .8), and response hand (left vs right, at the response onset time). A separate regressor denoted the occurence of invalid spatial cue trials. **The main contrasts of interest were *motoric: left hand > right hand*, *spatial attention: cue probability x cue laterality*, *relative value: $[H_{TGT}/L_{DST} > L_{TGT}/H_{DST}] > [H_{TGT}/H_{DST} > L_{TGT}/L_{DST}]***. Whole brain GLMs were fit using SPM12, using the `EstimateModel` Nipype wrapper. The resulting beta images and the residual variance estimate (ResMS.nii) were used to estimate contrast-noise ratio (CNR) values.

**CNR Estimates**
We estimated CNR defined as equation 2 in Welvaert & Rosseel [-@Welvaert2013-zj]. Specifically:

<br>

$CNR = \frac{A}{\sigma_N}$

<br>

where $A$ is the amplitude of the signal, which in the current case is the estimated beta weights from the contrasts of interest, and the noise of the signal $\sigma_N$, which is here defined as the square root of the residual variance estimate. Although the CNR values were computed for every voxel, the CNR was next extracted from the *a priori* defined regions of interest for further analysis. 

<br>

**Regions of Interest**

_Nuclei of the Basal Ganglia_: ROIs from the basal ganglia included the caudate nucleus (CN), the ventral striatum (VS), putamen (P), globus pallidus external (GPe), globus pallidus internal (GPi), and the sub-thalamic nucleus (STN). These were identifed using a published atlas derived from high-resolution 7T anatomical imaging [@Keuken2015-w], including the manual division of the striatum into the CN, P, and VS reported in [@Puckett2018-wy]. For each ROI, the left and right images were joined to form a bilateral mask image. 
_Cortical ROIs_: The cortical regions of interest were identified using the Neurosynth meta-analytic association test from  [topic 303](https://neurosynth.org/analyses/topics/v4-topics-400/303), of the set [v4-topics-400](https://neurosynth.org/analyses/topics/v4-topics-400/), which contains the top-loading topic terms: attention, orienting, spatial, attentional, cued, target, cue, cueing and endogenous cues. This meta-analysis identified 5 ROIs including the left and right frontal eye fields (FEF), left and right intra-parietal sulcus (IPS), and the left lateral occipital complex (LOC). The approximate centre of each ROI was defined in MNI co-ordinates, and a sphere with a radius of 5 mm was defined around the co-ordinates using `fslmaths`. The resulting images were binarised and served as the cortical masks.

Each mask was transformed from MNI space to participant space using the ANTs `ApplyTransforms` wrapper, where the mask files were transformed to the space of the participant's preprocessed T1w image, using the reverse of the transforms provided by fMRIprep [**from-MNI152NLin2009cAsym_to_t1w_mode-image_xfm.h5*]. Each mask file in participant space was then resliced (using SPM's `Reslice` wrapper) to match the dimensions of the CNR images from each T2* protocol (P1, P2, P3). The resulting resliced mask files were then used to extract the CNR data from the ROIs using FSLs `ApplyMask` wrapper.

CNR data from each ROI were extracted using `spm_read_vols` (to get the x, y, z coordinates for each ROI) and `spm_get_data` (to extract the CNR from the given coordinates). As the primary concern is regarding the magnitude rather than the direction of the CNR values (which is driven by the sign of the $A$ parameter), the square root of the mean squares was taken across voxels for each ROI. This shall now be referred to as the root mean square contrast-noise-ratio (RMS-CNR). 

<br>

### Statistical Approach

<br>

All subsequent analysis was conducted using R (version 3.3.2, [@R_Core_Team2013-au]) and RStudio (version 1.1.456, [@RStudio_Team2016-jj]). To assess how protocol influenced signal qulaity in the basal ganglia ROIs, the RMS-CNR values from each contrast of interest were subject to a repeated measures ANOVA with protocol (P1, P2 & P3) and ROI (CN, Put, VS, GPe, GPi and STN) as within-participant factors. A comparable repeated measures ANOVA was also performed for the data from the cortical ROIs, in which case the ROI factor had 3 levels (FEF, IPS, LOC).

# Results

<br>

## Behaviour

```{r loadbehdata, echo=FALSE, message=F, warning=F}
# source the data wrangling functions
source('../R/data_wrangles.R')
subjects = c(1, 2, 3, 4, 5)
sessions = c(1)
data_path = "~/Dropbox/MC-Projects/imaging-value-cert-att/striwp1"
raw.data <- get_participant_data(subjects, sessions, data_path, folstr="behav")

# Show it:
# raw.data %>% head(5)
```

```{r cleanbehdata, echo=FALSE, message=F, warning=F}

# clean sub data (remove incorrect responses, and resps > 3 sds above the participant median)
sub.data <- lapply(subjects, clean.sub.data, data = raw.data)
sub.data <- do.call(rbind, sub.data)  
# compute accuracy and inverse efficiency respectively from the two data frames (raw.data and sub.data), 
acc.data = raw.data %>% group_by(sub, reward_type, cert) %>%
            summarise(acc = mean(resp))
sum.inv.eff = sub.data %>% group_by(sub, reward_type, cert) %>%
              summarise(RT = median(rt)) %>%
              inner_join(acc.data, sum.inv.eff, by=c("sub", "reward_type", "cert")) %>%
              transform(inv_eff = RT/acc)
```

```{r plotbeh, warning=FALSE, fig.align='center', out.width="600pix", out.height="300pix", fig.cap="Showing RT, Accuracy and Inverse Efficiency scores for each subject"}
fig.cols = wes_palette("IsleofDogs1")[4:1]
IE <- plot.behav(sum.inv.eff, dv="inv_eff", iv="cert", grp="reward_type", ylims =c(0.4, 2), cols = fig.cols) + xlab("") + ylab("IE") + 
                scale_y_continuous(breaks=pretty_breaks(n=5)) + theme_cowplot()
RT <- plot.behav(sum.inv.eff, dv="RT", iv="cert", grp="reward_type", ylims =c(0.4, 0.9), cols = fig.cols) + xlab("cue certainty") +
                scale_y_continuous(breaks=pretty_breaks(n=5)) + theme_cowplot()
ACC <- plot.behav(sum.inv.eff, dv="acc", iv="cert", grp="reward_type", ylims =c(0, 1), cols = fig.cols) + xlab("") + ylab("ACC") + 
                scale_y_continuous(breaks=pretty_breaks(n=5)) + theme_cowplot()

# arrange plots
prow <- plot_grid(
  RT + theme(legend.position = "none"),
  ACC + theme(legend.position = "none"),
  IE + theme(legend.position = "none"), 
  align = 'vh',
  labels=c("A", "B", "C"),
  hjust=-1,
  nrow=1
)
# extract the legend from one of the plots
legend <- get_legend(
  # create some space to the left of the legend
  RT + theme(legend.box.margin = margin(0, 0, 0, 12))
)

# add the legend to the row we made earlier. Give it one-third of 
# the width of one plot (via rel_widths).
plot_grid(prow, legend, rel_widths = c(3, .5))

# note: refer to previous analysis
```


For each participant, individual RT data were analysed with trial as a regressor. 
```{r, warning=FALSE}

sub.model <- function(data, csub){
  data = data %>% filter(sub == csub)
  mod <- with(data, aov( rt ~ cert*reward_type+Error(factor(t)) ))
  mod
} 

incls = lapply( unique(sub.data$sub), sub.model, data = sub.data  )
lapply(as.numeric(unique(sub.data$sub)), function(x) summary(incls[[x]]))

```

```{r loadmribeh, warning=FALSE, echo=FALSE, message=F, warning=F}

sessions = c(2, 3, 4)
TRs = c(700, 1510, 1920)
mri.beh.raw <- get_mri_data(subjects, sessions, data_path, TRs)

```
```{r cleanmribeh, echo=FALSE, message=F, warning=F}
RT_min = .2
sd_reject = 2.5
mri.beh.clean <- mri.beh.raw %>% group_by(sub, sess, TR, reward_type, cert) %>%
                                 filter(rt > RT_min) %>%
                                 filter(resp == 1) %>%
                                 filter(rt < median(rt) + sd_reject*sd(rt)) 
```

```{r, mriIE, echo = FALSE, message=F, warning=F}

mri.acc = mri.beh.raw %>% group_by(sub, TR, reward_type, cert) %>%
                          summarise(acc = mean(resp))
mri.inv.eff = mri.beh.clean %>% group_by(sub, TR, reward_type, cert) %>%
                           summarise(RT = median(rt)) %>%
                           inner_join(mri.acc, sum.inv.eff, by=c("sub", "TR", "reward_type", "cert")) %>%
                           transform(inv_eff = RT/acc)
```

```{r, plotmribeh, warning=FALSE, fig.align='center', out.width="600pix", fig.cap="Showing Inverse Efficiency scores by reward condition and cue probability for each TR"}

plot.mri(mri.inv.eff, dv="inv_eff", iv="cert", grp="reward_type", facet="TR", ylims =c(0.4, 1), cols = fig.cols) + ylab("IE") + xlab("cue certainty")
```


<br>

## CNR

<br>

```{r CNR_by_reg, fig.align='center', out.width="600pix", fig.cap="CNR across a subset of ROIs. A) Cue certainty contrast, B) Response hand contrast"}
paradigm.fig.pth <- '../images/Fig_CNR_CN.png' 
#paradigm.fig <- readPNG(paradigm.fig.pth, native=TRUE, info=TRUE)
include_graphics(paradigm.fig.pth)

```
 <br>

### Comparing CNR Between Sequences

<br>

```{r loadCNRdat, echo=FALSE, message=F, warning=F}

# Load and tidy data
CNR = read.csv('~/Dropbox/documents/MC-Docs/seqtest-writeup/data/tThresh_agg.csv')
CNR$sub <- factor(CNR$sub)
CNR$TR <- factor(CNR$TR)
CNR$roi <- factor(CNR$roi)
CNR$contrast <- factor(CNR$contrast)
names(CNR)[names(CNR) == "tT"] = "R"

# CNR$roi <- CNR$roi %>% recode('1' = 'CN',
#                           '2' = 'FEF',
#                               '3' = 'GPe',
#                               '4' = 'GPi',
#                               '5' = 'IPS',
#                               '6' = 'LOC',
#                               '7' = 'Put', 
#                               '8' = 'STN',
#                               '9' = 'VS') 
CNR$contrast <- CNR$contrast %>%  recode('1' = 'tgtLoc',
                                         '2' = 'cueP',
                                         '3' = 'cueP x tgtLoc',
                                         '5' = 'AValue',
                                         '6' = 'HvL',
                                         '7' = 'RelValue',
                                         '9' = 'hand')

CNR <- CNR %>% mutate(roi=factor(roi, levels = c('FEF', 'IPS', 'LOC', 'VS', 'CN', 'Put', 'GPe', 'GPi', 'STN')))
```

```{r, boxplot, echo=FALSE}

with(CNR, boxplot(R~contrast*roi*TR))

# is it subject 1?
with(CNR[CNR$sub != "1", ], boxplot(R~contrast*roi*TR))
# subject 1 is > 3 sdevs from the mean on all measures, so excluding from the CNR analysis
CNR <- CNR %>% filter(sub != "1") 
```

```{r, out.width="600px", fig.align='center', fig.cap="Showing the RMS CNR values for each subject, TR sequence, and ROI for the main effect of response hand (LvsR)"}

draw.sub.plts <- function(data, C){
  
  blanks = data %>% filter(contrast==C) %>%
                    group_by(roi) %>%
                    summarise(min=min(R),
                              max=max(R))
  
  data %>% filter(contrast == C) %>%
           ggplot(aes(x=TR, y=R, group=sub)) +
           geom_line(aes(color=sub), lwd=1.1) + facet_wrap(.~roi, scales="free_y") +
           scale_colour_manual(values=c(wes_palette("Darjeeling2"))) +
           scale_y_continuous(breaks=pretty_breaks(n=4)) +
           scale_x_discrete("P", labels=c("1","2","3")) +
           labs(y="RMS CNR") + 
           theme_cowplot() +
           theme(strip.background=element_blank(), strip.placement="outside",
                 axis.title.x = element_text(face="italic"),
                 axis.title.y = element_text(face="italic")) 


}

draw.sub.plts(CNR, C="hand") 
ggsave("hand_RMS_CNR.pdf", dpi=300)
```

```{r, handstats, echo=FALSE}

# this code shows the main effect of TR, and then shows the followup comparisions
library(car)
regs = c('VS', 'CN', 'Put', 'GPe', 'GPi', 'STN') # get subcortical ROIs
hand.dat <- CNR %>% filter(roi %in% regs & contrast == "hand")
omni <- Anova(lm(R ~ TR * roi, data=hand.dat, contrasts=list(topic=contr.sum, sys=contr.sum)), type=3)
FUa <- Anova(lm(R ~ TR * roi, data=hand.dat %>% filter(TR %in% c("700", "1510")), contrasts=list(topic=contr.sum, sys=contr.sum)), type=3)
FUb <- Anova(lm(R ~ TR * roi, data=hand.dat %>% filter(TR %in% c("700", "1920")), contrasts=list(topic=contr.sum, sys=contr.sum)), type=3)
FUc <- Anova(lm(R ~ TR * roi, data=hand.dat %>% filter(TR %in% c("1510", "1920")), contrasts=list(topic=contr.sum, sys=contr.sum)), type=3)

# follow up FUb
lapply(regs, function(x) with(hand.dat %>% filter(TR %in% c("700", "1920") & roi==x), t.test(R[TR=="700"], R[TR=="1920"])))
detach("package:car",unload=TRUE)

```


```{r, out.width="600px", fig.align='center', fig.cap="Showing the RMS CNR values for each subject, TR sequence, and ROI for the interaction effect of cue probability (p=.5 vs .8) and target location (left vs right)"}
draw.sub.plts(CNR, C="tgtLoc")
ggsave("Tgt_RMS_CNR.pdf", dpi=300)
```
```{r, tgt_loc_stats, echo=FALSE}

library(car)
regs = c('VS', 'CN', 'Put', 'GPe', 'GPi', 'STN') # get subcortical ROIs
tgt.dat <- CNR %>% filter(roi %in% regs & contrast == "tgtLoc")
omni.tgt <- Anova(lm(R ~ TR * roi, data=tgt.dat, contrasts=list(topic=contr.sum, sys=contr.sum)), type=3)
detach("package:car",unload=TRUE)
```


```{r, load_and_tidy_FIR_dat}

FIR = read.csv('~/Dropbox/MC-Projects/imaging-value-cert-att/striwp1/FIR-data/FIR_data_across_subs.csv')

FIR$sub <- as.factor(FIR$sub)
FIR$TR <- as.factor(FIR$TR)
FIR$reg <- as.factor(FIR$reg)
FIR$condition <- as.factor(FIR$condition)
levels(FIR$condition) <- c("l","r")

regs <- c("FEF", "IPS", "LOC", "CN", "GPe", "GPi", "Put", "VS")
# separately extracting STN, as insufficient voxels to have tertiarty split, 
# therefore using all of them
muSTN <- FIR %>% filter(reg == "STN") %>%
                 group_by(TR, reg, condition, order) %>%
                 summarise(beta = mean(value, na.rm=T))
muFIR <- FIR %>% filter(reg %in% regs) %>%
                 filter(third == 3) %>%
                 group_by(TR, reg, condition, order) %>%
                 summarise(beta = mean(value))
muFIR <- rbind(muFIR, muSTN)
```

```{r, out.width="600px", fig.align='center', fig.cap="Showing the FIR function for each subject, TR sequence, ROI and condition for the top 33.3% of voxels"}

muFIR %>%  ggplot(aes(x=order, y=beta, group=condition)) +
           geom_line(aes(color=condition), lwd=1.1) + facet_grid(vars(TR), vars(reg)) +
           xlab("t") + 
           scale_x_continuous(breaks=seq(2,18,by=2),
                              labels = c("2","","","","10","","","","18")) +
           scale_y_continuous(breaks=seq(-0.025, .1, by = 0.025),
           labels=c("","0","","","",".1")) +
           scale_colour_manual(values=c(wes_palette("Royal2")[c(1:2)])) +
           ylab(expression(beta)) + theme_cowplot() 

ggsave("FIR_response.pdf", dpi=300)
```


<br>

# Supplemental Material

```{r, out.width="600px", fig.align='center', fig.cap="Showing the RMS CNR values for each subject, TR sequence, and ROI for the main effect of target location (left vs right)"}
cs <- levels(CNR$contrast)
draw.sub.plts(CNR, C=cs[1])
```
```{r, out.width="600px", fig.align='center', fig.cap="Showing the RMS CNR values for each subject, TR sequence, and ROI for the interaction effect of cue probability (p=.5 vs .8) and target location (left vs right)"}
draw.sub.plts(CNR, C=cs[3])
```


```{r, out.width="600px", fig.align='center', fig.cap="Showing the RMS CNR values for each subject, TR sequence, and ROI for the main effect of cue probability (p=.5 vs .8)"}
draw.sub.plts(CNR, C="cueP")
```

```{r, out.width="600px", fig.align='center', fig.cap="Showing the RMS CNR values for each subject, TR sequence, and ROI for the main effect of Relative Value (low tgt/high dst vs opposite)"}
draw.sub.plts(CNR, C=cs[5])
```

